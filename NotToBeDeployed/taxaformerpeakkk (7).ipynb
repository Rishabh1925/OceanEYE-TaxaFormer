{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14025758,"sourceType":"datasetVersion","datasetId":8932107},{"sourceId":14027288,"sourceType":"datasetVersion","datasetId":8932841},{"sourceId":14027457,"sourceType":"datasetVersion","datasetId":8932929},{"sourceId":14031821,"sourceType":"datasetVersion","datasetId":8935379},{"sourceId":14031833,"sourceType":"datasetVersion","datasetId":8935388},{"sourceId":14031857,"sourceType":"datasetVersion","datasetId":8935407},{"sourceId":14067112,"sourceType":"datasetVersion","datasetId":8953872}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers peft biopython pandas scikit-learn numpy tqdm","metadata":{"_uuid":"de6c22b2-57c4-4815-9c78-3d09e1feadea","_cell_guid":"2512241e-8947-46cc-8025-40700bbd8fde","trusted":true,"collapsed":false,"scrolled":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y protobuf\n!pip install protobuf==3.20.3","metadata":{"_uuid":"77c4a326-6af3-43bf-9386-466960daf8cb","_cell_guid":"c1fd1754-3a1c-4b9d-9d1d-402c1fcb981e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport transformers\nprint(\"ok!\")","metadata":{"_uuid":"98208e8a-7c26-4aee-8245-0dbc7c9da47b","_cell_guid":"36274d93-d7fd-40d0-b4f5-30f3b0334782","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nINPUT_FASTA = \"/kaggle/input/test-fasta/Sara_phaeo_field_161223_2d_assembly_1374_1871_V2.fasta\"  \nFINE_TUNED_MODEL_PATH = \"/kaggle/input/fine-tuned-model\" \nREF_EMBEDDINGS_PATH = \"/kaggle/input/finetuned-npy/embeddings_finetuned.npy\"\nREF_IDS_PATH = \"/kaggle/input/finetuned-npy/ids_finetuned.npy\"\nREF_TAXONOMY_PATH = \"/kaggle/input/normalized-tsv/combined_normalized_taxonomy.tsv\"\n\nOUTPUT_CSV = \"final_taxonomy_report.csv\"\nNOVELTY_CSV = \"novel_candidates_list.csv\"\n\nMODEL_BASE = \"InstaDeepAI/nucleotide-transformer-v2-100m-multi-species\"\nWINDOW_SIZE = 512\nSTRIDE = 256\nBATCH_SIZE = 16\nK_NEIGHBORS = 5  \nNOVELTY_THRESHOLD = 0.15","metadata":{"_uuid":"049b63d6-0504-495f-8362-91d4500ec4b3","_cell_guid":"ae8cdc5f-cd69-4c74-ba21-298f3f179679","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport re\nfrom Bio import SeqIO\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom peft import PeftModel\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import normalize\n\n# 1. NORMALIZATION\ndef normalize_seq(seq):\n    \"\"\"Standardizes DNA: U->T, Uppercase, Remove non-ACGTN.\"\"\"\n    s = seq.replace(\"U\", \"T\").replace(\"u\", \"t\").upper()\n    s = \"\".join(s.split()) \n    s = re.sub(r\"[^ACGTN]\", \"N\", s)\n    return s\n\n# 2. CHUNKING \ndef chunk_sequence(seq, window_size, stride):\n    \"\"\"Splits long sequences into overlapping windows.\"\"\"\n    chunks = []\n    for i in range(0, len(seq), stride):\n        chunk = seq[i : i + window_size]\n        if len(chunk) > 50: \n            chunks.append(chunk)\n        if i + window_size >= len(seq): break\n    return chunks\n\n# 3. AI INFERENCE ENGINE \ndef generate_embeddings(fasta_path, model_path, base_model_name):\n    print(f\"Loading AI Model from: {model_path}\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n    base_model = AutoModelForMaskedLM.from_pretrained(base_model_name, trust_remote_code=True)\n    \n    model = PeftModel.from_pretrained(base_model, model_path)\n    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.eval()\n    \n    embeddings = []\n    ids = []\n    \n    print(f\"Processing {fasta_path}...\")\n    for record in tqdm(SeqIO.parse(fasta_path, \"fasta\")):\n        clean_seq = normalize_seq(str(record.seq))\n        header = record.id\n        \n        chunks = chunk_sequence(clean_seq, WINDOW_SIZE, STRIDE)\n        if not chunks: continue\n\n        inputs = tokenizer(chunks, return_tensors=\"pt\", padding=True, truncation=True, max_length=WINDOW_SIZE)\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n        \n        with torch.no_grad():\n            outputs = model(**inputs, output_hidden_states=True)\n            hidden_states = outputs.hidden_states[-1]\n            attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n            masked_hidden = hidden_states * attention_mask\n            chunk_embs = masked_hidden.sum(dim=1) / attention_mask.sum(dim=1).clamp(min=1e-9)\n            final_seq_emb = chunk_embs.mean(dim=0).cpu().numpy()\n            \n        embeddings.append(final_seq_emb)\n        ids.append(header)\n        \n    return np.array(embeddings), ids","metadata":{"_uuid":"974428d3-396f-4233-bbf6-27435e499cec","_cell_guid":"ba1dd344-dc0e-4cb4-b147-3d406be3965e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastapi uvicorn python-multipart pyngrok","metadata":{"_uuid":"73c793a9-5f8d-4b02-bc9c-cc4154a98376","_cell_guid":"b97f5c91-753b-4e23-bd68-479566e3f4bb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pyngrok import ngrok\n\nNGROK_TOKEN = \"348roSQj2iERV8fMgVaCYElBgfB_4yPs4jKrwU4U323bzpmJL\" \n\nngrok.set_auth_token(NGROK_TOKEN)","metadata":{"_uuid":"3cb7f05d-1eff-4e9d-af64-5ba26622f861","_cell_guid":"14175242-01b3-4274-81a1-97f37caa88b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1. Install Dependencies\n!pip install -q transformers peft biopython pandas scikit-learn numpy fastapi uvicorn python-multipart pyngrok nest-asyncio\n\n#2. Fix Protobuf\n!pip uninstall -y protobuf\n!pip install protobuf==3.20.3\n\n#3. Imports\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport torch\nimport nest_asyncio\nfrom pyngrok import ngrok\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom peft import PeftModel\nfrom Bio import SeqIO\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import normalize\nimport uvicorn\nfrom fastapi import FastAPI, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\n\nnest_asyncio.apply()\n\nprint(\"Environment Ready.\")","metadata":{"_uuid":"7bfd05fe-2739-477b-8653-7b04ac56291c","_cell_guid":"b3a499d1-b7e7-4d23-9013-88d89b31f8fc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATHS = {\n    \"MODEL_BASE\": \"InstaDeepAI/nucleotide-transformer-v2-100m-multi-species\",\n    \"FINE_TUNED_MODEL\": \"/kaggle/input/fine-tuned-model\", # Path to your fine-tuned weights folder\n    \"REF_EMBEDDINGS\": \"/kaggle/input/finetuned-npy/embeddings_finetuned.npy\",            # Your saved embeddings .npy\n    \"REF_IDS\": \"/kaggle/input/finetuned-npy/ids_finetuned.npy\",                          # Your saved IDs .npy\n    \"REF_TAXONOMY\": \"/kaggle/input/normalized-tsv/combined_normalized_taxonomy.tsv\"       # Your taxonomy .tsv\n}\n\nPARAMS = {\n    \"WINDOW_SIZE\": 512,\n    \"STRIDE\": 256,\n    \"K_NEIGHBORS\": 5,\n    \"NOVELTY_THRESHOLD\": 0.15\n}\n\ndef load_knowledge_base():\n    print(\"Loading Knowledge Base...\")\n    \n    # 1. Load Files\n    try:\n        ref_emb = np.load(PATHS[\"REF_EMBEDDINGS\"])\n        ref_ids = np.load(PATHS[\"REF_IDS\"])\n        ref_tax_df = pd.read_csv(PATHS[\"REF_TAXONOMY\"], sep='\\t')\n    except FileNotFoundError as e:\n        return None, None, f\"Error: {e}\"\n\n    # 2. Align Data (ID Matching)\n    clean_ref_ids = [str(x).split('|')[0].replace('>', '').strip() for x in ref_ids]\n    ref_df_ids = pd.DataFrame({'accession': clean_ref_ids, 'idx': range(len(clean_ref_ids))})\n    \n    ref_tax_df['accession'] = ref_tax_df['header'].astype(str).str.split().str[0].replace('>', '')\n    \n    merged = pd.merge(ref_df_ids, ref_tax_df, on='accession', how='inner')\n\n    valid_indices = merged['idx'].values\n    valid_embs = ref_emb[valid_indices]\n    valid_taxa = merged['taxonomy'].values\n    \n    #3. Build Search Engine (k-NN)\n    print(\"Building Search Engine...\")\n    ref_emb_norm = normalize(valid_embs, axis=1)\n    knn_engine = NearestNeighbors(n_neighbors=PARAMS[\"K_NEIGHBORS\"], metric='cosine', n_jobs=-1)\n    knn_engine.fit(ref_emb_norm)\n    \n    print(f\"Knowledge Base Loaded! ({len(valid_taxa)} sequences)\")\n    return knn_engine, valid_taxa, \"Success\"\n\nknn_engine, ref_taxa, status = load_knowledge_base()\nif \"Error\" in status: print(status)","metadata":{"_uuid":"e650a27c-cff1-4209-831b-5b6d9af82b8b","_cell_guid":"16029465-824d-42fb-b9a4-f1c35bb30c4e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re","metadata":{"_uuid":"851dc1e0-028a-4165-82e3-72a19d8e3b90","_cell_guid":"defa1079-2a45-4288-afeb-a1b8702033b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Force install specific compatible versions\n!pip install -U -q \"bitsandbytes>=0.48.0\" \"transformers>=4.57.0\" \"accelerate>=1.0.0\" \"peft>=0.10.0\"\n\nprint(\"Libraries installed.\")\nprint(\"NOW: Go to the Menu Bar -> 'Runtime' (or 'Kernel') -> 'Restart Session' (or 'Restart Kernel').\")\nprint(\"DO NOT proceed until you have restarted!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers accelerate scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import bitsandbytes as bnb\nimport transformers\n\nprint(f\"BitsAndBytes Version: {bnb.__version__}\")\nprint(f\"Transformers Version: {transformers.__version__}\")\n\n# Check if CUDA (GPU) is available - 4-bit ONLY works on GPU\nimport torch\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\n# 1. Define the missing path variable\nFINE_TUNED_DIR = \"/kaggle/input/fine-tuned-model\" \n\n# 2. Configure 4-bit Quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\nprint(\"Loading Base Model (4-bit)...\")\nbase_model = AutoModelForMaskedLM.from_pretrained(\n    \"InstaDeepAI/nucleotide-transformer-v2-100m-multi-species\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\nprint(f\"Loading Fine-Tuned Adapters from: {FINE_TUNED_DIR}\")\nmodel = PeftModel.from_pretrained(base_model, FINE_TUNED_DIR)\ntokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_DIR, trust_remote_code=True)\nmodel.eval()\n\nprint(\"‚úÖ Model loaded successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\n# --- CONFIGURATION FOR QUANTIZATION ---\n# This converts weights to 4-bit integers to save memory and increase throughput\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,  # Compute in fast 16-bit\n    bnb_4bit_quant_type=\"nf4\",             # Normalized float 4 (better for pre-trained models)\n    bnb_4bit_use_double_quant=True         # Compresses constants for more savings\n)\n\nprint(\"Loading Fine-Tuned Model with 4-bit Quantization...\")\n\n# 1. Load Base Model (Quantized)\nbase_model = AutoModelForMaskedLM.from_pretrained(\n    \"InstaDeepAI/nucleotide-transformer-v2-100m-multi-species\",\n    quantization_config=bnb_config,        # <--- Apply Quantization here\n    device_map=\"auto\",                     # <--- Automatically map to GPU\n    trust_remote_code=True\n)\n\n# 2. Load your LoRA Adapters\n# Note: We do NOT use .to(\"cuda\") because device_map handled it\nmodel = PeftModel.from_pretrained(base_model, FINE_TUNED_DIR)\ntokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_DIR, trust_remote_code=True)\n\nmodel.eval()\nprint(\"‚úÖ Model loaded in 4-bit mode!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport transformers\nimport bitsandbytes\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\n# Verify versions match what we just installed\nprint(f\"Transformers Version: {transformers.__version__} (Should be >= 4.57)\")\nprint(f\"BitsAndBytes Version: {bitsandbytes.__version__} (Should be >= 0.48)\")\n\n# --- CONFIGURATION ---\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\nprint(\"\\nLoading Base Model in 4-bit...\")\nbase_model = AutoModelForMaskedLM.from_pretrained(\n    \"InstaDeepAI/nucleotide-transformer-v2-100m-multi-species\",\n    quantization_config=bnb_config,  # 4-bit optimization\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\nprint(\"Loading LoRA Adapters...\")\n# Replace 'FINE_TUNED_DIR' with your actual path (e.g., \"/kaggle/input/fine-tuned-model\")\nFINE_TUNED_DIR = \"/kaggle/input/fine-tuned-model\" \n\nmodel = PeftModel.from_pretrained(base_model, FINE_TUNED_DIR)\ntokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_DIR, trust_remote_code=True)\nmodel.eval()\n\nprint(\"Step 1 Complete: Model loaded efficiently!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\nclass BiodiversityPipeline:\n    def __init__(self, base_model_path, ft_model_path, knn_engine, ref_taxa):\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"Loading AI Model on {self.device}...\")\n        \n        # Load Tokenizer & Model\n        self.tokenizer = AutoTokenizer.from_pretrained(ft_model_path, trust_remote_code=True)\n        base_model = AutoModelForMaskedLM.from_pretrained(base_model_path, trust_remote_code=True)\n        self.model = PeftModel.from_pretrained(base_model, ft_model_path)\n        self.model.to(self.device)\n        self.model.eval()\n        \n        self.knn_engine = knn_engine\n        self.ref_taxa = ref_taxa\n        print(\"Pipeline Initialized (Optimized: 4-bit, Deduplication, Smart Sorting).\")\n\n    def _normalize_seq(self, seq):\n        s = seq.replace(\"U\", \"T\").replace(\"u\", \"t\").upper()\n        s = \"\".join(s.split())\n        return re.sub(r\"[^ACGTN]\", \"N\", s)\n\n    def _chunk_sequence(self, seq):\n        chunks = []\n        for i in range(0, len(seq), PARAMS[\"STRIDE\"]):\n            chunk = seq[i : i + PARAMS[\"WINDOW_SIZE\"]]\n            if len(chunk) > 50: chunks.append(chunk)\n            if i + PARAMS[\"WINDOW_SIZE\"] >= len(seq): break\n        return chunks\n\n    def process_file(self, fasta_path, batch_size=32):\n        print(f\"Reading {fasta_path}...\")\n        \n        # --- PHASE 1: DEDUPLICATION (CPU) ---\n        unique_seq_map = defaultdict(list)\n        total_reads = 0\n        \n        for record in SeqIO.parse(fasta_path, \"fasta\"):\n            total_reads += 1\n            seq = self._normalize_seq(str(record.seq))\n            unique_seq_map[seq].append(record.id)\n            \n        unique_sequences = list(unique_seq_map.keys())\n        print(f\"Pruning: Reduced {total_reads} reads to {len(unique_sequences)} unique sequences.\")\n        \n        # --- PHASE 2: PREPARE SMART BATCHES ---\n        # We create tuples: (chunk_text, unique_seq_index)\n        flat_chunks = []\n        \n        for idx, seq in enumerate(unique_sequences):\n            chunks = self._chunk_sequence(seq)\n            for chunk in chunks:\n                flat_chunks.append((chunk, idx))\n                \n        # SORT by length (Crucial for Smart Batching)\n        flat_chunks.sort(key=lambda x: len(x[0]))\n        \n        print(f\"Processing {len(flat_chunks)} chunks (Sorted by length)...\")\n        \n        # --- PHASE 3: BATCH INFERENCE (GPU) ---\n        # Dictionary to collect embeddings: { unique_seq_idx: [emb1, emb2...] }\n        seq_embeddings = defaultdict(list)\n        \n        # Iterate through the sorted list\n        for i in range(0, len(flat_chunks), batch_size):\n            batch_items = flat_chunks[i : i + batch_size]\n            batch_texts = [x[0] for x in batch_items]\n            batch_indices = [x[1] for x in batch_items]\n            \n            # Tokenize (Dynamic Padding - pads only to longest in THIS batch)\n            inputs = self.tokenizer(\n                batch_texts, \n                return_tensors=\"pt\", \n                padding=True, \n                truncation=True, \n                max_length=PARAMS[\"WINDOW_SIZE\"]\n            )\n            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n            \n            with torch.no_grad():\n                outputs = self.model(**inputs, output_hidden_states=True)\n                hidden = outputs.hidden_states[-1]\n                mask = inputs[\"attention_mask\"].unsqueeze(-1)\n                \n                # Mean Pool\n                chunk_embs = (hidden * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n                chunk_embs = chunk_embs.cpu() # Move to CPU\n                \n                # Distribute back to the correct unique sequence\n                for k, emb in enumerate(chunk_embs):\n                    seq_idx = batch_indices[k]\n                    seq_embeddings[seq_idx].append(emb)\n\n        # --- PHASE 4: AGGREGATE & IDENTIFY ---\n        results = []\n        print(\"Matching & Expanding Results...\")\n        \n        for idx, seq_str in enumerate(unique_sequences):\n            embs = seq_embeddings.get(idx)\n            if not embs: continue\n            \n            # Average chunks to get Sequence Embedding\n            # Stack list of tensors -> Tensor -> Mean\n            final_emb = torch.stack(embs).mean(dim=0).numpy().reshape(1, -1)\n            \n            # KNN Search\n            query_norm = normalize(final_emb, axis=1)\n            dists, indices = self.knn_engine.kneighbors(query_norm)\n            \n            avg_dist = float(np.mean(dists[0]))\n            status = \"Known\" if avg_dist < PARAMS[\"NOVELTY_THRESHOLD\"] else \"POTENTIALLY NOVEL\"\n            neighbors = self.ref_taxa[indices[0]]\n            predicted_tax = max(set(neighbors), key=list(neighbors).count)\n            \n            # Copy result to all original IDs\n            for original_id in unique_seq_map[seq_str]:\n                results.append({\n                    \"sequence_id\": original_id,\n                    \"status\": status,\n                    \"novelty_score\": round(avg_dist, 4),\n                    \"taxonomy\": predicted_tax\n                })\n            \n        return results\n\n# Initialize Pipeline with the optimized class\npipeline = BiodiversityPipeline(PATHS[\"MODEL_BASE\"], PATHS[\"FINE_TUNED_MODEL\"], knn_engine, ref_taxa)","metadata":{"_uuid":"e595056c-1a8f-4ae1-8bf2-5432cf92c987","_cell_guid":"d3b4c6fe-136b-49c5-84b9-456d7d782faa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nSetup script for Kaggle notebook - Database Caching\nRun this in your Kaggle notebook to set up database caching\n\"\"\"\n\n# Step 1: Install required packages\nprint(\"üì¶ Installing required packages...\")\nimport subprocess\nimport sys\n\ndef install_package(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\ntry:\n    install_package(\"supabase\")\n    print(\"‚úÖ Supabase installed\")\nexcept Exception as e:\n    print(f\"‚ùå Failed to install supabase: {e}\")\n\n# Step 2: Set environment variables\nimport os\nos.environ[\"SUPABASE_URL\"] = \"https://nbnyhdwbnxbheombbhtv.supabase.co\"\nos.environ[\"SUPABASE_KEY\"] = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5ibnloZHdibnhiaGVvbWJiaHR2Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjU0MDIyNDksImV4cCI6MjA4MDk3ODI0OX0.u5DxN1eX-K85WepTNCEs5sJw9M13YLmGm5pVe1WKy34\"\nos.environ[\"USE_DATABASE\"] = \"true\"\n\nprint(\"‚úÖ Environment variables set\")\n\n# Step 3: Test database connection\nprint(\"üß™ Testing database connection...\")\ntry:\n    from supabase import create_client\n    \n    client = create_client(\n        os.environ[\"SUPABASE_URL\"],\n        os.environ[\"SUPABASE_KEY\"]\n    )\n    \n    # Test query\n    response = client.table('analysis_jobs').select('*').limit(1).execute()\n    print(\"‚úÖ Database connection successful!\")\n    print(f\"üìä Found {len(response.data)} existing records\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Database connection failed: {e}\")\n\n# Step 4: Ready message\nprint(\"\\nüöÄ SETUP COMPLETE!\")\nprint(\"=\" * 50)\nprint(\"‚úÖ Supabase package installed\")\nprint(\"‚úÖ Environment variables configured\")\nprint(\"‚úÖ Database connection tested\")\nprint(\"\\nüí° Now run your backend with database caching!\")\nprint(\"üîÑ Caching will work automatically!\")\nprint(\"   ‚Ä¢ First upload: Processes and stores in DB\")\nprint(\"   ‚Ä¢ Same file again: Returns cached result instantly\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T05:55:46.932833Z","iopub.execute_input":"2025-12-11T05:55:46.933580Z","iopub.status.idle":"2025-12-11T05:55:54.402327Z","shell.execute_reply.started":"2025-12-11T05:55:46.933529Z","shell.execute_reply":"2025-12-11T05:55:54.401527Z"}},"outputs":[{"name":"stdout","text":"üì¶ Installing required packages...\nCollecting supabase\n  Downloading supabase-2.25.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting realtime==2.25.1 (from supabase)\n  Downloading realtime-2.25.1-py3-none-any.whl.metadata (7.0 kB)\nCollecting supabase-functions==2.25.1 (from supabase)\n  Downloading supabase_functions-2.25.1-py3-none-any.whl.metadata (2.4 kB)\nCollecting storage3==2.25.1 (from supabase)\n  Downloading storage3-2.25.1-py3-none-any.whl.metadata (2.1 kB)\nCollecting supabase-auth==2.25.1 (from supabase)\n  Downloading supabase_auth-2.25.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting postgrest==2.25.1 (from supabase)\n  Downloading postgrest-2.25.1-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\nCollecting deprecation>=2.1.0 (from postgrest==2.25.1->supabase)\n  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\nRequirement already satisfied: pydantic<3.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from postgrest==2.25.1->supabase) (2.12.4)\nRequirement already satisfied: yarl>=1.20.1 in /usr/local/lib/python3.11/dist-packages (from postgrest==2.25.1->supabase) (1.22.0)\nRequirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.11/dist-packages (from realtime==2.25.1->supabase) (4.15.0)\nRequirement already satisfied: websockets<16,>=11 in /usr/local/lib/python3.11/dist-packages (from realtime==2.25.1->supabase) (15.0.1)\nRequirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase) (2.10.1)\nCollecting strenum>=0.4.15 (from supabase-functions==2.25.1->supabase)\n  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation>=2.1.0->postgrest==2.25.1->supabase) (25.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest==2.25.1->supabase) (4.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.9->postgrest==2.25.1->supabase) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.9->postgrest==2.25.1->supabase) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0,>=1.9->postgrest==2.25.1->supabase) (0.4.2)\nRequirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase) (46.0.3)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl>=1.20.1->postgrest==2.25.1->supabase) (6.7.0)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl>=1.20.1->postgrest==2.25.1->supabase) (0.4.1)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\nRequirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase) (2.0.0)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.25.1->supabase) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest==2.25.1->supabase) (4.1.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth==2.25.1->supabase) (2.23)\nDownloading supabase-2.25.1-py3-none-any.whl (16 kB)\nDownloading postgrest-2.25.1-py3-none-any.whl (21 kB)\nDownloading realtime-2.25.1-py3-none-any.whl (22 kB)\nDownloading storage3-2.25.1-py3-none-any.whl (26 kB)\nDownloading supabase_auth-2.25.1-py3-none-any.whl (48 kB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 48.0/48.0 kB 2.8 MB/s eta 0:00:00\nDownloading supabase_functions-2.25.1-py3-none-any.whl (8.5 kB)\nDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\nDownloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\nInstalling collected packages: strenum, deprecation, realtime, supabase-functions, supabase-auth, storage3, postgrest, supabase\nSuccessfully installed deprecation-2.1.0 postgrest-2.25.1 realtime-2.25.1 storage3-2.25.1 strenum-0.4.15 supabase-2.25.1 supabase-auth-2.25.1 supabase-functions-2.25.1\n‚úÖ Supabase installed\n‚úÖ Environment variables set\nüß™ Testing database connection...\n‚úÖ Database connection successful!\nüìä Found 0 existing records\n\nüöÄ SETUP COMPLETE!\n==================================================\n‚úÖ Supabase package installed\n‚úÖ Environment variables configured\n‚úÖ Database connection tested\n\nüí° Now run your backend with database caching!\nüîÑ Caching will work automatically!\n   ‚Ä¢ First upload: Processes and stores in DB\n   ‚Ä¢ Same file again: Returns cached result instantly\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nTaxaformer Backend API with Database Caching\nFastAPI server with file hash-based idempotency and Supabase storage\n\"\"\"\nimport os\nimport sys\nimport shutil\nimport json\nimport hashlib\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional\nimport uvicorn\nfrom fastapi import FastAPI, File, UploadFile, HTTPException, Form\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pyngrok import ngrok\n\n# Import your existing pipeline\nfrom pipeline import TaxonomyPipeline\n\n# Database imports\ntry:\n    from supabase import create_client, Client\n    \n    class TaxaformerDB:\n        def __init__(self):\n            self.url = os.getenv(\"SUPABASE_URL\")\n            self.key = os.getenv(\"SUPABASE_KEY\")\n            print(f\"üîó Connecting to Supabase: {self.url}\")\n            self.client = create_client(self.url, self.key)\n        \n        def compute_file_hash(self, file_bytes: bytes) -> str:\n            return hashlib.sha256(file_bytes).hexdigest()\n        \n        def get_job_by_hash(self, file_hash: str):\n            try:\n                response = self.client.table('analysis_jobs').select('*').eq('file_hash', file_hash).limit(1).execute()\n                return response.data[0] if response.data else None\n            except Exception as e:\n                print(f\"Error getting job by hash: {e}\")\n                return None\n        \n        def store_analysis(self, file_hash: str, filename: str, result_json: Dict[str, Any]) -> str:\n            try:\n                data = {\n                    \"file_hash\": file_hash,\n                    \"filename\": filename,\n                    \"status\": \"complete\",\n                    \"result\": result_json,\n                    \"completed_at\": datetime.utcnow().isoformat()\n                }\n                \n                response = self.client.table('analysis_jobs').insert(data).execute()\n                job_id = response.data[0]['job_id']\n                \n                # Store sequences\n                if \"sequences\" in result_json:\n                    sequence_records = []\n                    for seq in result_json[\"sequences\"]:\n                        record = {\n                            \"job_id\": job_id,\n                            \"accession\": seq.get(\"accession\"),\n                            \"taxonomy\": seq.get(\"taxonomy\"),\n                            \"length\": seq.get(\"length\"),\n                            \"confidence\": seq.get(\"confidence\"),\n                            \"overlap\": seq.get(\"overlap\"),\n                            \"cluster\": seq.get(\"cluster\"),\n                            \"novelty_score\": seq.get(\"novelty_score\"),\n                            \"status\": seq.get(\"status\")\n                        }\n                        sequence_records.append(record)\n                    \n                    if sequence_records:\n                        self.client.table('sequences').insert(sequence_records).execute()\n                \n                return job_id\n            except Exception as e:\n                print(f\"Error storing analysis: {e}\")\n                raise\n    \n    # Initialize database\n    db = TaxaformerDB()\n    print(\"‚úÖ Supabase database connected\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Database not available: {e}\")\n    db = None\n\n# Initialize FastAPI app\napp = FastAPI(\n    title=\"Taxaformer API\",\n    description=\"Taxonomic analysis pipeline with caching\",\n    version=\"1.1.0\"\n)\n\n# Configure CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Initialize pipeline\npipeline = TaxonomyPipeline()\n\n# Directory for temporary files\nTEMP_DIR = \"temp_uploads\"\nos.makedirs(TEMP_DIR, exist_ok=True)\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"status\": \"online\",\n        \"service\": \"Taxaformer API\",\n        \"version\": \"1.1.0\",\n        \"database\": \"connected\" if db else \"disabled\",\n        \"caching\": True if db else False,\n        \"timestamp\": datetime.utcnow().isoformat()\n    }\n\n@app.post(\"/analyze\")\nasync def analyze_endpoint(\n    file: UploadFile = File(...),\n    metadata: Optional[str] = Form(None)\n):\n    temp_filepath = None\n    \n    try:\n        # Validate file\n        if not file.filename:\n            raise HTTPException(status_code=400, detail=\"No filename provided\")\n        \n        allowed_extensions = ['.fasta', '.fa', '.fastq', '.fq', '.txt']\n        file_ext = os.path.splitext(file.filename)[1].lower()\n        if file_ext not in allowed_extensions:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Unsupported file type. Allowed: {', '.join(allowed_extensions)}\"\n            )\n        \n        # Read file bytes for hashing\n        file_bytes = await file.read()\n        \n        if db:\n            file_hash = db.compute_file_hash(file_bytes)\n            print(f\"üìÅ File: {file.filename} ({len(file_bytes)} bytes)\")\n            print(f\"üîç Hash: {file_hash[:16]}...\")\n            \n            # Check cache\n            cached_job = db.get_job_by_hash(file_hash)\n            if cached_job and cached_job.get('status') == 'complete':\n                print(f\"üíæ Cache HIT: Returning cached result for job {cached_job['job_id']}\")\n                return {\n                    \"status\": \"success\",\n                    \"job_id\": cached_job[\"job_id\"],\n                    \"cached\": True,\n                    \"data\": cached_job[\"result\"]\n                }\n        \n        # Save file temporarily for processing\n        temp_filepath = os.path.join(TEMP_DIR, f\"temp_{datetime.now().timestamp()}_{file.filename}\")\n        with open(temp_filepath, \"wb\") as buffer:\n            buffer.write(file_bytes)\n        \n        print(f\"üî¨ Processing file: {file.filename}\")\n        \n        # Process file through pipeline\n        start_time = datetime.now()\n        result_data = pipeline.process_file(temp_filepath, file.filename)\n        processing_time = (datetime.now() - start_time).total_seconds()\n        \n        # Add processing time\n        if \"metadata\" in result_data:\n            result_data[\"metadata\"][\"processingTime\"] = f\"{processing_time:.2f}s\"\n        \n        print(f\"‚úÖ Analysis complete: {file.filename} ({processing_time:.2f}s)\")\n        \n        # Store in database if available\n        job_id = None\n        if db:\n            try:\n                job_id = db.store_analysis(file_hash, file.filename, result_data)\n                print(f\"üíæ Saved to database with job_id: {job_id}\")\n            except Exception as db_error:\n                print(f\"‚ö†Ô∏è Database save failed: {db_error}\")\n        \n        # Return response\n        response = {\n            \"status\": \"success\",\n            \"cached\": False,\n            \"data\": result_data\n        }\n        \n        if job_id:\n            response[\"job_id\"] = job_id\n        \n        return response\n        \n    except HTTPException:\n        raise\n    except Exception as e:\n        print(f\"‚ùå Error processing file: {str(e)}\")\n        return {\n            \"status\": \"error\",\n            \"message\": f\"Analysis failed: {str(e)}\"\n        }\n    finally:\n        if temp_filepath and os.path.exists(temp_filepath):\n            try:\n                os.remove(temp_filepath)\n            except Exception as e:\n                print(f\"Warning: Could not delete temp file: {e}\")\n\n@app.get(\"/health\")\nasync def health_check():\n    return {\n        \"status\": \"healthy\",\n        \"pipeline\": \"initialized\",\n        \"database\": \"connected\" if db else \"disabled\",\n        \"caching\": True if db else False,\n        \"temp_dir\": os.path.exists(TEMP_DIR),\n        \"timestamp\": datetime.utcnow().isoformat()\n    }\n\n@app.get(\"/jobs\")\nasync def list_jobs(limit: int = 50):\n    if not db:\n        raise HTTPException(status_code=503, detail=\"Database not available\")\n    \n    try:\n        response = (db.client.table('analysis_jobs')\n                   .select('job_id, filename, status, created_at, completed_at')\n                   .order('created_at', desc=True)\n                   .limit(limit)\n                   .execute())\n        return {\"jobs\": response.data or []}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\ndef start_server(port: int = 8000, use_ngrok: bool = True, ngrok_token: str = None):\n    if use_ngrok:\n        if not ngrok_token:\n            raise ValueError(\"ngrok_token is required when use_ngrok=True\")\n        \n        ngrok.set_auth_token(ngrok_token)\n        \n        try:\n            tunnels = ngrok.get_tunnels()\n            for tunnel in tunnels:\n                print(f\"Closing existing tunnel: {tunnel.public_url}\")\n                ngrok.disconnect(tunnel.public_url)\n        except Exception as e:\n            print(f\"Note: {e}\")\n        \n        try:\n            public_url = ngrok.connect(port).public_url\n            print(\"\\n\" + \"=\"*60)\n            print(\"üöÄ TAXAFORMER API STARTED (WITH CACHING)\")\n            print(\"=\"*60)\n            print(f\"üì° PUBLIC URL: {public_url}\")\n            print(f\"üîß LOCAL URL:  http://localhost:{port}\")\n            print(f\"üíæ DATABASE:   {'Connected' if db else 'Disabled'}\")\n            print(f\"üîÑ CACHING:    {'Enabled' if db else 'Disabled'}\")\n            print(\"=\"*60)\n            print(f\"\\n‚ö° Copy the PUBLIC URL to your frontend!\")\n            print(f\"   Update API_URL to: {public_url}\")\n            print(\"\\nüîÑ Caching Features:\")\n            print(\"   ‚Ä¢ First upload: Processes and stores in DB\")\n            print(\"   ‚Ä¢ Same file again: Returns cached result instantly\")\n            print(\"\\n\" + \"=\"*60 + \"\\n\")\n        except Exception as e:\n            print(f\"\\n‚ùå Failed to create ngrok tunnel: {e}\")\n            raise\n    else:\n        print(f\"\\nüöÄ Server starting on http://localhost:{port}\")\n        print(f\"üíæ DATABASE: {'Connected' if db else 'Disabled'}\")\n    \n    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n\nif __name__ == \"__main__\":\n    # Configuration\n    NGROK_TOKEN = \"348roSQj2iERV8fMgVaCYElBgfB_4yPs4jKrwU4U323bzpmJL\"\n    PORT = 8000\n    USE_NGROK = True\n    \n    # Start server\n    start_server(port=PORT, use_ngrok=USE_NGROK, ngrok_token=NGROK_TOKEN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T06:01:56.858919Z","iopub.execute_input":"2025-12-11T06:01:56.859904Z","iopub.status.idle":"2025-12-11T06:01:57.168884Z","shell.execute_reply.started":"2025-12-11T06:01:56.859875Z","shell.execute_reply":"2025-12-11T06:01:57.167885Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_12326/1379943886.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Import your existing pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaxonomyPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Database imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pipeline'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pipeline'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"import uvicorn\nimport shutil\nimport os\nimport nest_asyncio\nfrom fastapi import FastAPI, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pyngrok import ngrok\n\n# 1. Apply Asyncio patch (Required for Colab/Kaggle)\nnest_asyncio.apply()\n\n# 2. Define the API App\napp = FastAPI()\n\n# Enable CORS so your website can talk to this server\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n@app.post(\"/analyze\")\nasync def analyze_endpoint(file: UploadFile = File(...)):\n    \"\"\"\n    Receives a FASTA file, processes it with the OPTIMIZED pipeline, \n    and returns the taxonomy results.\n    \"\"\"\n    temp_filename = f\"temp_{file.filename}\"\n    try:\n        # Save uploaded file temporarily\n        with open(temp_filename, \"wb\") as buffer:\n            shutil.copyfileobj(file.file, buffer)\n\n        # --- RUN THE OPTIMIZED PIPELINE ---\n        # This uses the 'pipeline' object we created in the previous step\n        print(f\"Received file: {file.filename}\")\n        data = pipeline.process_file(temp_filename, batch_size=32)\n        \n        return {\"status\": \"success\", \"data\": data}\n        \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {\"status\": \"error\", \"message\": str(e)}\n    finally:\n        # Clean up temp file\n        if os.path.exists(temp_filename):\n            os.remove(temp_filename)\n\n# 3. Start the Server\n# Set your Ngrok Token\nNGROK_TOKEN = \"348roSQj2iERV8fMgVaCYElBgfB_4yPs4jKrwU4U323bzpmJL\" \nngrok.set_auth_token(NGROK_TOKEN)\n\n# Open a Tunnel\npublic_url = ngrok.connect(8000).public_url\nprint(f\"\\nSERVER IS LIVE!\")\nprint(f\"PUBLIC API URL: {public_url}\")\nprint(\"Copy this URL and paste it into your website's JavaScript code.\\n\")\n\n# Run the App\nuvicorn.run(app, host=\"0.0.0.0\", port=8000)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-11T05:49:46.379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install supabase","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fastapi import FastAPI, File, UploadFile, Form\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom supabase import create_client, Client\nimport json\nimport shutil\nimport os\nimport uvicorn\nfrom pyngrok import ngrok\nfrom datetime import datetime\nfrom uuid import uuid4\n\n# --------------------------------\n# 1. INIT FASTAPI\n# --------------------------------\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# --------------------------------\n# 2. CONNECT TO SUPABASE\n# --------------------------------\nurl: str = \"https://hdzzhfcgyvqsqoghjewz.supabase.co\"\nkey: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhkenpoZmNneXZxc3FvZ2hqZXd6Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjUxNjU0MTksImV4cCI6MjA4MDc0MTQxOX0.ki_x3n6hVmdhZPBqUYeYaRKgt1oYzciS68YwVlCnR6Y\"\n\nsupabase: Client = create_client(url, key)\n\n# --------------------------------\n# 3. ANALYZE ENDPOINT\n#    (FILE + METADATA)\n# --------------------------------\n@app.post(\"/analyze\")\nasync def analyze_endpoint(\n    file: UploadFile = File(...),\n    metadata: str = Form(None)\n):\n    temp_filename = f\"temp_{uuid4()}_{file.filename}\"  # safer unique temp file\n\n    try:\n        # Save uploaded FASTA temporarily\n        with open(temp_filename, \"wb\") as buffer:\n            shutil.copyfileobj(file.file, buffer)\n\n        # Parse metadata JSON safely\n        try:\n            metadata_dict = json.loads(metadata) if metadata else {}\n        except json.JSONDecodeError:\n            metadata_dict = {\"warning\": \"invalid JSON metadata\"}\n\n        # --------------------------------\n        # RUN YOUR PIPELINE \n        # (Make sure 'pipeline' is imported or defined in your full code)\n        # --------------------------------\n        data = pipeline.process_file(temp_filename)\n        \n\n        # --------------------------------\n        # SAVE RESULT IN SUPABASE\n        # --------------------------------\n        job_id = str(uuid4())\n       # Prepare the row to insertaa\n        # NOTE: Supabase stores JSON in 'jsonb' columns perfectly\n        document = {\n            \"id\": job_id,\n            \"filename\": file.filename,\n            \"metadata\": metadata_dict,\n            \"analysis_result\": data,\n            \"created_at\": datetime.utcnow().isoformat()\n        }\n\n        # Insert into table 'analysis_results'\n        response = supabase.table(\"analysis_results\").insert(document).execute()\n\n        # --------------------------------\n        # RETURN IMMEDIATE JSON TO FRONTEND\n        # --------------------------------\n        return {\n            \"status\": \"success\",\n            \"job_id\": job_id,\n            \"data\": data\n        }\n\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n    finally:\n        if os.path.exists(temp_filename):\n            os.remove(temp_filename)\n\n\n# --------------------------------\n# 4. FETCH RESULTS BY ID\n# --------------------------------\n@app.get(\"/result/{job_id}\")\nasync def get_result(job_id: str):\n    try:\n        # Select * from 'analysis_results' where id matches job_id\n        response = supabase.table(\"analysis_results\").select(\"*\").eq(\"id\", job_id).execute()\n        \n        if (result.job_id) :\n          localStorage.setItem('currentJobId', result.job_id)\n        \n\n        # response.data is a list of results\n        if not response.data:\n            return {\"status\": \"error\", \"message\": \"Result not found.\"}\n            \n        return {\"status\": \"success\", \"data\": response.data[0]}\n        \n    except Exception as e:\n        return {\"status\": \"error\", \"message\": str(e)}\n\n\n# --------------------------------\n# 5. NGROK SETUP (KAGGLE)\n# --------------------------------\nNGROK_TOKEN = \"348roSQj2iERV8fMgVaCYElBgfB_4yPs4jKrwU4U323bzpmJL\"\nngrok.set_auth_token(NGROK_TOKEN)\n\npublic_url = ngrok.connect(8000).public_url\nprint(\"PUBLIC API URL:\", public_url)\nprint(\"Put this URL in your website fetch() calls\")\n\nuvicorn.run(app, host=\"0.0.0.0\", port=8000)","metadata":{"trusted":true,"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### ","metadata":{}}]}